{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo #installed ucimlrepo using pip command to import the package ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZtQd2hrexl5B",
        "outputId": "55a4a67c-2330-439f-d20a-ce97fec2b045"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Belows given codes with some already done basic pre-processing"
      ],
      "metadata": {
        "id": "7BAzU9Mu_531"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n",
        "X_initial_shape = print(X.shape)                                                 #added to know the shape of dataframes(X and y ) before dropping\n",
        "y_initial_shape = print(y.shape)\n",
        "\n",
        "\n",
        "print(X.columns)                                                                 #to know the features and target columns\n",
        "print(y.columns)\n",
        "\n",
        "X = X.drop(['fnlwgt', 'education-num'],axis=1)                                   #dropped these two columns\n",
        "\n",
        "y = y[~X.isna().any(axis=1)]\n",
        "X = X[~X.isna().any(axis=1)]\n",
        "\n",
        "X_shape_after_dropping = print(X.shape)                                          #added to know the shape of dataframes(X and y) after dropping\n",
        "y_shape_after_dropping = print(y.shape)\n",
        "\n",
        "columns = list(X.columns)\n",
        "for col in columns:\n",
        "\tcol_list = list(X[col])\n",
        "\tif isinstance(col_list[0], int):\n",
        "\t\tprint('Column name: `{}\\', type: integer'.format(col))\n",
        "\t\tprint('')\n",
        "\telse:\n",
        "\t\tprint('Column name: `{}\\', type: categorical'.format(col))\n",
        "\t\tprint('\\tPossible values: {}'.format(list(set(col_list))))\n",
        "\t\tprint('')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pQoRS_gXW4Qu",
        "outputId": "c59f7977-2fca-4737-9078-bed1ea645500"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48842, 14)\n",
            "(48842, 1)\n",
            "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
            "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
            "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'],\n",
            "      dtype='object')\n",
            "Index(['income'], dtype='object')\n",
            "(47621, 12)\n",
            "(47621, 1)\n",
            "Column name: `age', type: integer\n",
            "\n",
            "Column name: `workclass', type: categorical\n",
            "\tPossible values: ['Never-worked', 'Self-emp-inc', '?', 'Local-gov', 'Without-pay', 'Federal-gov', 'Private', 'State-gov', 'Self-emp-not-inc']\n",
            "\n",
            "Column name: `education', type: categorical\n",
            "\tPossible values: ['Assoc-voc', 'Prof-school', 'Some-college', 'Assoc-acdm', '5th-6th', '10th', 'Preschool', '12th', '11th', 'Doctorate', '7th-8th', '1st-4th', '9th', 'HS-grad', 'Bachelors', 'Masters']\n",
            "\n",
            "Column name: `marital-status', type: categorical\n",
            "\tPossible values: ['Divorced', 'Married-civ-spouse', 'Married-spouse-absent', 'Separated', 'Widowed', 'Never-married', 'Married-AF-spouse']\n",
            "\n",
            "Column name: `occupation', type: categorical\n",
            "\tPossible values: ['Armed-Forces', '?', 'Adm-clerical', 'Other-service', 'Machine-op-inspct', 'Transport-moving', 'Craft-repair', 'Farming-fishing', 'Priv-house-serv', 'Exec-managerial', 'Sales', 'Tech-support', 'Protective-serv', 'Prof-specialty', 'Handlers-cleaners']\n",
            "\n",
            "Column name: `relationship', type: categorical\n",
            "\tPossible values: ['Own-child', 'Husband', 'Wife', 'Other-relative', 'Not-in-family', 'Unmarried']\n",
            "\n",
            "Column name: `race', type: categorical\n",
            "\tPossible values: ['Asian-Pac-Islander', 'Black', 'Other', 'White', 'Amer-Indian-Eskimo']\n",
            "\n",
            "Column name: `sex', type: categorical\n",
            "\tPossible values: ['Female', 'Male']\n",
            "\n",
            "Column name: `capital-gain', type: integer\n",
            "\n",
            "Column name: `capital-loss', type: integer\n",
            "\n",
            "Column name: `hours-per-week', type: integer\n",
            "\n",
            "Column name: `native-country', type: categorical\n",
            "\tPossible values: ['Mexico', '?', 'Holand-Netherlands', 'Iran', 'Portugal', 'Greece', 'Philippines', 'England', 'Canada', 'Ecuador', 'Honduras', 'Haiti', 'France', 'Hungary', 'Japan', 'Trinadad&Tobago', 'United-States', 'Puerto-Rico', 'Laos', 'Germany', 'Cambodia', 'Taiwan', 'China', 'Vietnam', 'Italy', 'Columbia', 'Ireland', 'Jamaica', 'Poland', 'Thailand', 'Dominican-Republic', 'Peru', 'Hong', 'Outlying-US(Guam-USVI-etc)', 'Cuba', 'Yugoslavia', 'El-Salvador', 'South', 'Guatemala', 'Nicaragua', 'India', 'Scotland']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations:\n",
        "- Dataset has been divided into X and y dataframes\n",
        "- The initial shape is :(48842, 14) , (48842, 1) respectively.\n",
        "\n",
        "- After dropping fnlwgt, education-num , we are left with (47621, 12),\n",
        "(47621, 1)\n",
        "\n",
        "- **to be noted that Few columsn are still not cleaned and contains dirty data such as '?'**\n"
      ],
      "metadata": {
        "id": "WEDtY2zXAX11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Data Cleaning\n",
        "-Decided to do more Data Cleaning as columns \"workclass\",\"occupation\"and \"native country\" have \"?\" ."
      ],
      "metadata": {
        "id": "xVZq6J4oW7Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values_X = X.isna().sum()                                       #trying to check null values in X and y dataframes\n",
        "print(missing_values_X)\n",
        "\n",
        "missing_values_y = y.isna().sum()\n",
        "print(\"Missing values in Traget column\",missing_values_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-MpjShB8W6w5",
        "outputId": "8f0be2d2-2cd7-449f-fc5c-aa24817002ba"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age               0\n",
            "workclass         0\n",
            "education         0\n",
            "marital-status    0\n",
            "occupation        0\n",
            "relationship      0\n",
            "race              0\n",
            "sex               0\n",
            "capital-gain      0\n",
            "capital-loss      0\n",
            "hours-per-week    0\n",
            "native-country    0\n",
            "dtype: int64\n",
            "Missing values in Traget column income    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.replace('?',np.NaN,inplace=True) # we don't have missing values in dataframes but we do have ? in few columns so , i am replacing with \"nan \" values"
      ],
      "metadata": {
        "id": "GsVnh273XlwE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y[~X.isna().any(axis=1)] # after replacing '?' with 'nan' values , i am not taking those in my X and y dataframes\n",
        "X = X[~X.isna().any(axis=1)]"
      ],
      "metadata": {
        "id": "ifNTs2D_b6EU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Again Checking for missing values for double surity\n",
        "missing_values_X = X.isna().sum()\n",
        "print(\"Missing values in each column of X after handling '?':\\n\", missing_values_X)\n",
        "missing_values_y = y.isna().sum()\n",
        "print(missing_values_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0-eEGRuvY_Yx",
        "outputId": "4dfcee9c-da24-4e8a-e8c4-2f44cc8f3b20"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column of X after handling '?':\n",
            " age               0\n",
            "workclass         0\n",
            "education         0\n",
            "marital-status    0\n",
            "occupation        0\n",
            "relationship      0\n",
            "race              0\n",
            "sex               0\n",
            "capital-gain      0\n",
            "capital-loss      0\n",
            "hours-per-week    0\n",
            "native-country    0\n",
            "dtype: int64\n",
            "income    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_shape = print(X.shape)\n",
        "y_shape = print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SDluhjFKcHAQ",
        "outputId": "3b3806de-c759-4425-e9d8-3bbb3e78ca56"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45222, 12)\n",
            "(45222, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "- Therefore , this leaves us with information of about 45222 people.:"
      ],
      "metadata": {
        "id": "iAtrI9LqHWG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.1\n",
        "***Strategize [15 pts] For each of the twelve pieces of demographic information, explain in 1-2 sentences how you will represent that piece of information as one or more real numbers and why. Note: there is not a single “right” way to do this!***\n",
        "\n",
        "\n",
        "# Answers :\n",
        "1. Age: We are going to represent as the integer value only because age is a continuous variable  can be used directly in its numerical form.\n",
        "\n",
        "2. Workclass: We are going to use one-hot encoding to represent each category as a binary feature. As this ensures no ordinal relationship is implied between categories.\n",
        "\n",
        "3. Education: Decided to apply one-hot encoding to each education level. Since education is a nominal categorical variable, this avoids implying any order among levels.\n",
        "\n",
        "4. Marital-Status: Going to use one-hot encoding for each marital status category because this takes each status as a distinct category without suggesting any hierarchy.\n",
        "\n",
        "5. Occupation: We will apply one-hot encoding to convert occupation into binary features. Each job role is distinct, and this method prevents false assumptions about relationships between them.\n",
        "\n",
        "6. Relationship: We will be using one-hot encoding to handle relationship categories. This will represent each type of relationship as an independent feature, avoiding any implied ranking.\n",
        "\n",
        "7. Race: Will apply one-hot encoding to represent each race category. This ensures that different races are treated independently without any ordinal assumptions.\n",
        "\n",
        "8. Sex: We will go with binary encoding (0 for Female, 1 for Male). With only two categories, binary encoding is efficient and sufficient.\n",
        "\n",
        "9. Capital-Gain: Will keep the integer value as it because Capital-gain is already a numeric feature and can be used directly.\n",
        "\n",
        "10. Capital-Loss: We will use the integer value as is. This variable is already numerical and requires no transformation.\n",
        "\n",
        "11. Hours-Per-Week: Use the raw integer value, as hours worked per week is a continuous variable.\n",
        "\n",
        "12. Native-Country: Will apply one-hot encoding to represent each country as a distinct binary feature. This avoids implying any geographical or economic ranking between countries.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NkQNaDSKDNjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.2 Implement [15 pts]\n",
        "Write code to implement the featurization strategy you just described. How many features do you have in total for each data instance?\n",
        "Note: I recommend including, at this stage, a feature that is always equal to 1 to account for the intercept term in the linear model we will train in the next problem."
      ],
      "metadata": {
        "id": "X2gNbG46dxeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()            # taking all the columns having datatype object from X and keeping it in  Categorical Cols\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()    # taking all the columns having datatype int or float from X and keeping it in  Numerical Cols\n",
        "\n",
        "for col in categorical_cols:\n",
        "    unique_categories = X[col].nunique()\n",
        "    print(f\"{col}: {unique_categories} unique categories \")\n",
        "\n",
        "print(\"x\" * 100)\n",
        "\n",
        "# Featurization for categorical\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')                         # One-Hot Encoding for Categorical features\n",
        "X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_cols]))\n",
        "\n",
        "# Featurization for numerical\n",
        "X_numerical = X[numerical_cols].reset_index(drop=True)                             # Selected the numerical columns from X dataframe and resetting the index to avoid index misalignment\n",
        "X_featurized = pd.concat([X_numerical, X_encoded.reset_index(drop=True)], axis=1)  # Concatenated the numerical features and the one-hot encoded categorical features into a single DataFrame\n",
        "\n",
        "scaler = StandardScaler()                                                          # Applyed standardization on the numerical features in X_featurized to normalize them\n",
        "X_featurized[numerical_cols] = scaler.fit_transform(X_featurized[numerical_cols])\n",
        "\n",
        "#Added Intercept Term\n",
        "X_featurized['intercept'] = 1                                                      # Adding a column of ones to X_featurized dataframe to serve as an intercept term for linear models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Shape of the featurized data:\", X_featurized.shape)\n",
        "print(\"First few rows of the featurized data:\\n\", X_featurized.head())\n",
        "\n",
        "\n",
        "\n",
        "total_features = X_featurized.shape[1]                                              # Total number of features\n",
        "print(\"Total number of features after transformation:\", total_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "V_o2mbT7d0Xi",
        "outputId": "fc986c67-7492-4f7a-f174-43e6e594bcd8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "workclass: 7 unique categories \n",
            "education: 16 unique categories \n",
            "marital-status: 7 unique categories \n",
            "occupation: 14 unique categories \n",
            "relationship: 6 unique categories \n",
            "race: 5 unique categories \n",
            "sex: 2 unique categories \n",
            "native-country: 41 unique categories \n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Shape of the featurized data: (45222, 95)\n",
            "First few rows of the featurized data:\n",
            "         age  capital-gain  capital-loss  hours-per-week    0    1    2    3  \\\n",
            "0  0.034201      0.142888      -0.21878       -0.078120  0.0  0.0  0.0  0.0   \n",
            "1  0.866417     -0.146733      -0.21878       -2.326738  0.0  0.0  0.0  1.0   \n",
            "2 -0.041455     -0.146733      -0.21878       -0.078120  0.0  1.0  0.0  0.0   \n",
            "3  1.093385     -0.146733      -0.21878       -0.078120  0.0  1.0  0.0  0.0   \n",
            "4 -0.798015     -0.146733      -0.21878       -0.078120  0.0  1.0  0.0  0.0   \n",
            "\n",
            "     4    5  ...   81   82   83   84   85   86   87   88   89  intercept  \n",
            "0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0          1  \n",
            "1  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0          1  \n",
            "2  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0          1  \n",
            "3  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0          1  \n",
            "4  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0          1  \n",
            "\n",
            "[5 rows x 95 columns]\n",
            "Total number of features after transformation: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "- Numerical Features: The count remains the same after normalization.\n",
        "- One-Hot Encoded Categorical Features: For each categorical column, the number of features is (number of unique categories - 1) because of drop='first'.\n",
        "- Intercept Term: Adds 1 to the total count."
      ],
      "metadata": {
        "id": "uIcg1MZdMQxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying to understand y dataframe more\n",
        "- since our target column is income , so we have checked for any kind of >,<,=,whitespaces\n",
        "- our goal is to predict yes/no whether\n",
        "their yearly income > $50,000\n",
        "- so for that we have to map income column into two parts (<=50k = -1 and >50k =1)\n",
        "- we need to check whether it's an imbalanced dataset or balanced before procedding ahead to get rid of any kind of baised behaviour."
      ],
      "metadata": {
        "id": "lP4XtMV2Mkri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(y, pd.DataFrame):\n",
        "    y = y['income']\n",
        "\n",
        "\n",
        "y = y.str.strip().str.replace('.', '', regex=False)                             # checked all extra whitespace or punctuation\n",
        "\n",
        "\n",
        "y_numeric = y.replace({'<=50K': -1, '>50K': 1})                                 # We are replacing the string labels ('<=50K' and '>50K') with numeric values -1 and 1\n",
        "\n",
        "\n",
        "print(\"Class distribution after converting labels:\")\n",
        "print(y_numeric.value_counts(normalize=True))                                   # this vaerifies the conversion of class labels into -1 and 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kqzK0eaSn5Gw",
        "outputId": "2262df41-7a2a-4218-c4ed-e10865101c9d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after converting labels:\n",
            "income\n",
            "-1    0.752156\n",
            " 1    0.247844\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-2186d00422b2>:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y_numeric = y.replace({'<=50K': -1, '>50K': 1})                                 # We are replacing the string labels ('<=50K' and '>50K') with numeric values -1 and 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations :\n",
        "1. Here, <=50K -> -1 : Approximately 75.2% of the instances in the dataset have an income less than or equal to $50K.\n",
        "\n",
        "2. and, >50K -> 1 : Approximately 24.8% of the instances have an income greater than $50K.\n",
        "\n",
        "3. This confirms that this dataset is imbalanced, with more people earning <=50K than >50K.\n",
        "\n",
        "4. Hence , we need to handle this imbalance to prevent it from biased predictions favoring the majority class.\n",
        "\n"
      ],
      "metadata": {
        "id": "agaUFp2TOuzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 Classify! [60 pts]\n",
        "Now that you have featurized the data, we will try to classify it using logistic regression.\n",
        "Split the data into training, validation, and testing data (roughly in 80% / 10% / 10% of the data\n",
        "in each)."
      ],
      "metadata": {
        "id": "oPIzTnrWfmsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_featurized, y_numeric, test_size=0.2,\n",
        "                                                    random_state=42, stratify=y_numeric) # Splitted the data into training (80%) and temporary (20%) sets\n",
        "\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5,\n",
        "                                                random_state=42, stratify=y_temp         # Splitted the temporary set into validation (10%) and testing (10%) sets\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Training set size:\", X_train.shape[0])                                            # to know the sizes of each sets train, test and validation set\n",
        "print(\"Validation set size:\", X_val.shape[0])\n",
        "print(\"Test set size:\", X_test.shape[0])\n",
        "\n",
        "\n",
        "print(\"\\nClass distribution in Training set:\")                                          # to verify class distribution in each split\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"\\nClass distribution in Validation set:\")\n",
        "print(y_val.value_counts(normalize=True))\n",
        "print(\"\\nClass distribution in Test set:\")\n",
        "print(y_test.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GnBowLSEfbgf",
        "outputId": "6ffa7d45-3326-4685-8828-8b31e7dc4b1a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 36177\n",
            "Validation set size: 4522\n",
            "Test set size: 4523\n",
            "\n",
            "Class distribution in Training set:\n",
            "income\n",
            "-1    0.752163\n",
            " 1    0.247837\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class distribution in Validation set:\n",
            "income\n",
            "-1    0.752101\n",
            " 1    0.247899\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class distribution in Test set:\n",
            "income\n",
            "-1    0.752156\n",
            " 1    0.247844\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Class Imbalance with Class Weights\n"
      ],
      "metadata": {
        "id": "Md_TsHF3oSZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate class weights based on training set\n",
        "classes = np.unique(y_train)                                                     # This gives you an array of all distinct classes present in the y_training data only(because model learns from training set)\n",
        "class_counts = y_train.value_counts().sort_index()                               # gives the count of each class labels having unique values\n",
        "total_samples = len(y_train)\n",
        "\n",
        "class_weights = {cls: total_samples / (len(classes) * count) for cls, count in class_counts.items()}\n",
        "\n",
        "print(\"\\nClass Weights:\")\n",
        "print(class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dJ-1YwAXoZDs",
        "outputId": "a9736070-f816-4409-85fc-49913f192ac6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class Weights:\n",
            "{-1: 0.6647495498144133, 1: 2.0174548293553425}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations\n",
        "1. `Class -1 with weight:0.6647`\n",
        "\n",
        "- This weight indicates that informations of this class are less critical for the model's learning process compared to the instances of class 1 .\n",
        "\n",
        "- Since this class has more examples in the dataset, the weight is less than which means that the model should not focus excessively on this class when making predictions.\n",
        "\n",
        "2. `Class 1 with weight 2.0174`\n",
        "\n",
        "- This weight is significantly greater than 1 suggesting that instances of this class are more critical for the model's learning.\n",
        "\n",
        "- Since this class has lower count compared to class -1 , assigning a higher weight helps the model pay more attention to correctly classifying these instances."
      ],
      "metadata": {
        "id": "UjSV1pqSofYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2.2 SGD [20 pts]\n",
        "Write code to implement stochastic gradient descent for minimizing the logistic loss."
      ],
      "metadata": {
        "id": "buHU-7tiZ-K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))                                                  # returns sigmoid of z\n",
        "\n",
        "def compute_gradient(X_i, y_i, w, lambda_reg, class_weights):         # Parameters:  X_i: Feature vector for sample i  , y_i: Label for sample i , w: Current weight vector, lambda_reg: Regularization parameter, class_weights: Dictionary mapping class labels to weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    z = y_i * np.dot(w, X_i)                                                     # Calculated  the linear combination of features and weights y_i * (w^T * X_i)\n",
        "    prediction = sigmoid(z)\n",
        "    gradient = -y_i * X_i * (1 - prediction)                                     # Calculated the gradient of the loss with respect to the weights\n",
        "\n",
        "                                                                                 # Applied class weights\n",
        "    weight = class_weights[y_i]\n",
        "    gradient *= weight                                                           # Scaled the gradient by the class weight\n",
        "\n",
        "\n",
        "    gradient += lambda_reg * w                                                   # L2 Regualarization term\n",
        "\n",
        "    return gradient                                                              # Returns Gradient vector\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OBx_U_w-gP2r"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the regularized logistic loss across all samples, taking into account the class weights.\n",
        "\n",
        "def compute_loss(X, y, w, lambda_reg, class_weights):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - X: Feature matrix (NumPy array)\n",
        "    - y: Labels (NumPy array, values -1 or 1)\n",
        "    - w: Weight vector (NumPy array)\n",
        "    - lambda_reg: Regularization parameter (float)\n",
        "    - class_weights: Dictionary mapping class labels to weights (dict)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    z = y * np.dot(X, w)                    #Calculating the linear combination of inputs and weights: z = y * (X * w)\n",
        "\n",
        "\n",
        "    loss_terms = np.log(1 + np.exp(-z))    # Computing the loss terms using the logistic loss formula: log(1 + exp(-z))\n",
        "\n",
        "\n",
        "    weighted_loss = 0                      # this is to hold the total weighted loss\n",
        "    for cls in class_weights:\n",
        "        cls_mask = (y == cls)\n",
        "        weighted_loss += np.sum(class_weights[cls] * loss_terms[cls_mask])\n",
        "\n",
        "\n",
        "    loss = weighted_loss / len(y)           # Average the total weighted loss\n",
        "\n",
        "\n",
        "    loss += (lambda_reg / 2) * np.sum(w ** 2)  # Add regularization\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "1tswVX6AowPL"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, w_init, lambda_reg, learning_rate, num_iterations, class_weights):\n",
        "\n",
        "    w = w_init.copy()                                                            # a copy of the initial weights to avoid modifying the original\n",
        "    m, n = X.shape                                                               # Get the number of samples (m) and features (n) from the feature matrix\n",
        "\n",
        "\n",
        "    for epoch in range(num_iterations):                                           # Loop through the specified number of iterations (epochs)\n",
        "        for i in range(m):\n",
        "            idx = np.random.randint(m)                                            # here update the model weights using few randomly selected samples m at each iteration.\n",
        "            X_i = X[idx]\n",
        "            y_i = y[idx]\n",
        "\n",
        "            gradient = compute_gradient(X_i, y_i, w, lambda_reg, class_weights)   # calling Compute gradient function to calculate gradient\n",
        "\n",
        "\n",
        "            w -= learning_rate * gradient                                         # Update weights using gradient and learning rate\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:                                                # Print loss every 100 epochs to moniter training loss\n",
        "            loss = compute_loss(X, y, w, lambda_reg, class_weights)\n",
        "            print(f\"Epoch {epoch+1}/{num_iterations}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return w                                                                      # RETURN Trained weight vector\n"
      ],
      "metadata": {
        "id": "BUTH4YZYo0WH"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Accuracy [10 pts]\n",
        "Recall that the logistic regression classifier outputs the probability that the correct label for an input\n",
        "is +1. Let’s say that the classifier is “correct” if the output it assigns to the correct label is at least\n",
        "50%, and the train/test “accuracy” is the percentage of the train/test set that the classifier gets\n",
        "correct.\n",
        "Write code to calculate the train and test set accuracy"
      ],
      "metadata": {
        "id": "Gjn-IeFYeuit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w):           # to Make predictions using the logistic regression model\n",
        "\n",
        "    probabilities = sigmoid(np.dot(X, w))\n",
        "    return np.where(probabilities >= 0.5, 1, -1)\n",
        "\n",
        "def calculate_accuracy(X, y, w):   # to Calculate the accuracy of the model\n",
        "\n",
        "    predictions = predict(X, w)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "Q5HTnfXSo2rf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2.4 Train! [25 pts]\n",
        "- Use your SGD code to train a logistic regression classifier.\n",
        "- Notice that there are three “hyperparameters”—the regularization parameter, λ, the stepsize, η, and the number of iterations of SGD (and there may actually be a fourth, the minibatch size, b, if\n",
        "you choose to do that)—that you need to choose.\n",
        "- You should make some amount of effort to choose these hyperparameters in order to get good performance out of your model, but don’t go too crazy.\n",
        "- Describe how you chose your hyperparameters(guess and check is fine, but describe how you “checked”).\n",
        "- Report the final test accuracy of the model that you learned. Recall, computing the test accuracy here should be the only time that you touch the test dataset.\n",
        "- The final test accuracy should be at least 60-70%, otherwise there is probably a bug in your code somewhere! If you’re feeling competitive, it should be possible to get test accuracy in the mid to\n",
        "high 80% range, but for full points you only need to get 60-70+% accuracy."
      ],
      "metadata": {
        "id": "fAvAb1cZue4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try 1 : Initialized parameters\n",
        "w_init = np.zeros(X_train.shape[1])\n",
        "lambda_reg = 1\n",
        "learning_rate = 1\n",
        "num_iterations = 100\n",
        "\n",
        "# Training the model using SGD\n",
        "w_final = stochastic_gradient_descent(\n",
        "    X_train.values,\n",
        "    y_train.values,\n",
        "    w_init,\n",
        "    lambda_reg,\n",
        "    learning_rate,\n",
        "    num_iterations,\n",
        "    class_weights\n",
        ")\n",
        "\n",
        "# Evaluating the Model\n",
        "train_accuracy = calculate_accuracy(X_train.values, y_train.values, w_final)\n",
        "val_accuracy = calculate_accuracy(X_val.values, y_val.values, w_final)\n",
        "test_accuracy = calculate_accuracy(X_test.values, y_test.values, w_final)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuplWFj15iXq",
        "outputId": "31b8713d-84be-464e-8119-7a7daa215115"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100, Loss: 0.8143\n",
            "\n",
            "Training Accuracy: 75.21%\n",
            "Validation Accuracy: 75.21%\n",
            "Test Accuracy: 75.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try 2 : Initialized parameters\n",
        "w_init = np.zeros(X_train.shape[1])\n",
        "lambda_reg = 0.01\n",
        "learning_rate = 0.01\n",
        "num_iterations = 100\n",
        "\n",
        "# Training the model using SGD\n",
        "w_final = stochastic_gradient_descent(\n",
        "    X_train.values,\n",
        "    y_train.values,\n",
        "    w_init,\n",
        "    lambda_reg,\n",
        "    learning_rate,\n",
        "    num_iterations,\n",
        "    class_weights\n",
        ")\n",
        "\n",
        "# Evaluating the Model\n",
        "train_accuracy = calculate_accuracy(X_train.values, y_train.values, w_final)\n",
        "val_accuracy = calculate_accuracy(X_val.values, y_val.values, w_final)\n",
        "test_accuracy = calculate_accuracy(X_test.values, y_test.values, w_final)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_YFGssDy4iFw",
        "outputId": "0b6c3145-df5f-4645-c106-cc535e13c956"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100, Loss: 0.4538\n",
            "\n",
            "Training Accuracy: 77.37%\n",
            "Validation Accuracy: 77.71%\n",
            "Test Accuracy: 77.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try 3 : Initialized parameters\n",
        "w_init = np.zeros(X_train.shape[1])\n",
        "lambda_reg = 0.01\n",
        "learning_rate = 0.001\n",
        "num_iterations = 500\n",
        "\n",
        "# Training the model using SGD\n",
        "w_final = stochastic_gradient_descent(\n",
        "    X_train.values,\n",
        "    y_train.values,\n",
        "    w_init,\n",
        "    lambda_reg,\n",
        "    learning_rate,\n",
        "    num_iterations,\n",
        "    class_weights\n",
        ")\n",
        "# Evaluating the Model\n",
        "train_accuracy = calculate_accuracy(X_train.values, y_train.values, w_final)\n",
        "val_accuracy = calculate_accuracy(X_val.values, y_val.values, w_final)\n",
        "test_accuracy = calculate_accuracy(X_test.values, y_test.values, w_final)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bn9-QXEm4pDu",
        "outputId": "75c0b33b-9e97-4818-902b-696881e249be"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/500, Loss: 0.4497\n",
            "Epoch 200/500, Loss: 0.4501\n",
            "Epoch 300/500, Loss: 0.4504\n",
            "Epoch 400/500, Loss: 0.4495\n",
            "Epoch 500/500, Loss: 0.4498\n",
            "\n",
            "Training Accuracy: 79.46%\n",
            "Validation Accuracy: 79.72%\n",
            "Test Accuracy: 79.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try 4 : Initialized parameters\n",
        "w_init = np.zeros(X_train.shape[1])\n",
        "lambda_reg = 0.001  # Lower regularization\n",
        "learning_rate = 0.01  # Increased learning rate\n",
        "num_iterations = 500  # Keep the same\n",
        "\n",
        "# Training the model using SGD\n",
        "w_final = stochastic_gradient_descent(\n",
        "    X_train.values,\n",
        "    y_train.values,\n",
        "    w_init,\n",
        "    lambda_reg,\n",
        "    learning_rate,\n",
        "    num_iterations,\n",
        "    class_weights\n",
        ")\n",
        "\n",
        "# Evaluating the Model\n",
        "train_accuracy = calculate_accuracy(X_train.values, y_train.values, w_final)\n",
        "val_accuracy = calculate_accuracy(X_val.values, y_val.values, w_final)\n",
        "test_accuracy = calculate_accuracy(X_test.values, y_test.values, w_final)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqo4LNboDtwY",
        "outputId": "5c70459a-fd22-4c04-cf88-fed0c159447e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/500, Loss: 0.4036\n",
            "Epoch 200/500, Loss: 0.4024\n",
            "Epoch 300/500, Loss: 0.4112\n",
            "Epoch 400/500, Loss: 0.4007\n",
            "Epoch 500/500, Loss: 0.4019\n",
            "\n",
            "Training Accuracy: 80.61%\n",
            "Validation Accuracy: 80.94%\n",
            "Test Accuracy: 80.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I chose a \"guess and check\" approach by adjusting one hyperparameter at a time which allowed me to isolate the effects of each change. After each adjustment, I evaluated the model's performance using accuracy metrics and loss values.\n",
        "\n",
        "\n",
        "- For the initial run (Try 1), I set the regularization parameter to a high value (1) and the learning rate to 1, which is generally not recommended but served as a baseline for assessing the model's performance.\n",
        "\n",
        "- I reduced the learning rate from 1 to 0.01 in Try 2 to allow the model to converge more effectively. As we can see , loss decreased in try2 to Loss: 0.4538.\n",
        "\n",
        "- I experimented with the number of iterations to see how increased training time affected accuracy. I moved from 100 to 500 in subsequent tries to observe whether more training epochs improved the model's ability to learn.\n"
      ],
      "metadata": {
        "id": "ZHK1NGDMACfN"
      }
    }
  ]
}